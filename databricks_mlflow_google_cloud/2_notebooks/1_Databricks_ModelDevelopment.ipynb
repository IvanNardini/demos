{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Managing the ModelLife Cycle with MLflow and GCP\n",
    "\n",
    "This notebook walks through the process of:\n",
    "\n",
    "    1. Training a PySpark model on Boston House Prices\n",
    "    2. Saving the model with MLflow (Mleap flavor)\n",
    "    3. Store Model in Github\n",
    "\n",
    "#### Author: \n",
    "\n",
    "**Nardini, Ivan - Sr. Customer Advisor | CI & Analytics Team | ModelOps & Decisioning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "    1. Launch a Python 3 cluster running Databricks Runtime 5.0\n",
    "    2. Install the MLeap Scala libraries (maven)\n",
    "    3. Install MLflow and MLeap libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cluster and install MLflow and MLeap on the cluster\n",
    "\n",
    "    1. Create a cluster specifying:\n",
    "      - Databricks Runtime Version: Databricks Runtime 5.0 or above\n",
    "      - Python Version: Python 3\n",
    "\n",
    "    2. Install required libraries: \n",
    "      - Create library with Source Maven Coordinate and the fully-qualified Maven artifact coordinate: ml.combust.mleap:mleap-spark_2.11:0.13.0\n",
    "      - Install the libraries into the cluster.\n",
    "\n",
    "    3. Install required Python library \n",
    "      - Create required library: Source PyPI and enter mlflow[extras].\n",
    "      - Install the libraries into the cluster.\n",
    "\n",
    "    4. Attach this notebook to the cluster.\n",
    "    \n",
    "**Notice**: You can install mlflow and mleap libraries from notebook as well. Below the commands\n",
    "\n",
    "    - dbutils.library.installPyPI(\"mlflow\", \"1.7.0\", extras=\"extras\")\n",
    "    - dbutils.library.installPyPI(\"mleap\", \"0.15.0\", extras=\"extras\")\n",
    "    - dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston House Prices\n",
    "-------------------\n",
    "[https://archive.ics.uci.edu/ml/machine-learning-databases/housing/]( https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)\n",
    "\n",
    "Contains information collected by the U.S. Census Service regarding housing in the Boston, Massachusetts area.\n",
    "\n",
    "Originally published by Harrison, D. and Rubinfeld, D.L. `Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
    "\n",
    "Rows: 506  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column|Type|Description        |\n",
    "|------| :---: |----------------|\n",
    "|crim|float|per capita crime rate by town|\n",
    "|zn|float|proportion of residential land zoned for lots over 25,000 sq.ft|\n",
    "|indus|float|proportion of non-retail business acres per town|\n",
    "|chas|int|Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)|\n",
    "|nox|float|nitric oxides concentration (parts per 10 million)|\n",
    "|rm|float|average number of rooms per dwelling|\n",
    "|age|float|proportion of owner-occupied units built prior to 1940|\n",
    "|dis|float|weighted distances to five Boston employment centres|\n",
    "|rad|float|index of accessibility to radial highways|\n",
    "|tax|float|full-value property-tax rate per 10,000 dollars|\n",
    "|ptratio|float|pupil-teacher ratio by town|\n",
    "|b|float|1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town|\n",
    "|lstat|float|% lower status of the population|\n",
    "|medv|float|median value of owner-occupied homes in 1000’s dollars|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark session\n",
    "\n",
    "In the Databricks notebook, when you create a cluster, the SparkSession is created for you. In both cases it’s accessible through a variable called spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.installPyPI(\"mlflow\", \"1.7.0\", extras=\"extras\")\n",
    "dbutils.library.installPyPI(\"mleap\", \"0.15.0\", extras=\"extras\")\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Machine Learning libraries\n",
    "import pyspark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "#Charts library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#MLflow\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow import log_metric,  log_artifact\n",
    "import mlflow.spark\n",
    "import mlflow.mleap\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "\n",
    "#utils\n",
    "import os\n",
    "from urllib import request\n",
    "import warnings\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request.urlretrieve(\"https://github.com/sassoftware/python-sasctl/raw/master/examples/data/boston_house_prices.csv\",\"/tmp/boston_house_prices.csv\")\n",
    "dbutils.fs.mv(\"file:/tmp/boston_house_prices.csv\",\"dbfs:/data/boston_house_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .option(\"HEADER\", True)\n",
    "  .option(\"inferSchema\", True)\n",
    "  .csv(\"/data/boston_house_prices.csv\")\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#median value of owner-occupied homes in $1000’s\n",
    "display(df[['medv']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#median value of owner-occupied homes in $1000’\n",
    "#average number of rooms per dwelling\n",
    "\n",
    "display(df[['medv', 'rm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at other relationships\n",
    "# crim - per capita crime rate by town\n",
    "# lower - % lower status of the population\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plotdf = df[[\"rm\", \"crim\", \"lstat\", \"medv\", \"rad\", \"tax\"]].toPandas()\n",
    "\n",
    "pd.plotting.scatter_matrix(plotdf)\n",
    "# ax.set_title('Scatter plot')\n",
    "\n",
    "display(fig.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate correlation\n",
    "\n",
    "assembler = VectorAssembler(inputCols=list(df.columns), outputCol=\"features\")\n",
    "df_ftz = assembler.transform(df)\n",
    "\n",
    "pearsonCorr = Correlation.corr(df_ftz, 'features').collect()\n",
    "\n",
    "corrdf = pd.DataFrame(pearsonCorr[0][0].toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrdf.index, corrdf.columns = df.columns, df.columns\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(corrdf)\n",
    "display(fig.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development and Model Tracking with Mlflow\n",
    "\n",
    "We will fit: \n",
    "\n",
    "  1. **Baseline Model** (by calculating the average housing value in the training dataset)\n",
    "\n",
    "and then we challenge it with \n",
    "\n",
    "  2. **Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test splitting\n",
    "train, test= df.randomSplit([0.7, 0.3], seed=12345)\n",
    "\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n",
    "\n",
    "train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline model\n",
    "\n",
    "fit = train.groupby().avg('medv').collect()[0][0]\n",
    "print(\"Average home value: {}\".format(fit))\n",
    "\n",
    "predict = test.withColumn(\"prediction\", lit(fit))\n",
    "display(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BaseModel\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"medv\")\n",
    "rmse = evaluator.evaluate(predict)\n",
    "mse = evaluator.evaluate(predict, {evaluator.metricName: \"mse\"})\n",
    "r2 = evaluator.evaluate(predict, {evaluator.metricName: \"r2\"})\n",
    "mae = evaluator.evaluate(predict, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(\"rmse on the test set for the baseline model: {}\".format(rmse))\n",
    "print(\"mse on the test set for the baseline model: {}\".format(mse))\n",
    "print(\"r2 on the test set for the baseline model: {}\".format(r2))\n",
    "print(\"mae on the test set for the baseline model: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the Baseline experiment\n",
    "\n",
    "with mlflow.start_run(run_name=\"Basic RF Experiment\") as run:\n",
    "  \n",
    "  # Log a metrics\n",
    "  log_metric(\"rmse\", rmse)\n",
    "  log_metric(\"mse\", mse)\n",
    "  log_metric(\"r2\", r2)\n",
    "  log_metric(\"mae\", mae)\n",
    "  \n",
    "  #Log artefacts (Scored Test data)\n",
    "  scored_df = predict.toPandas()\n",
    "  scored_df.to_csv('scored_df.csv')\n",
    "  log_artifact(\"scored_df.csv\")\n",
    "\n",
    "  runID = run.info.run_uuid\n",
    "  experimentID = run.info.experiment_id\n",
    "  \n",
    "  print(\"Inside MLflow Run with run_id {} and experiment_id {}\".format(runID, experimentID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.schema.names[:-1]\n",
    "assembler_features = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "abt_train = assembler_features.transform(train)\n",
    "abt_test = assembler_features.transform(test)\n",
    "\n",
    "#display\n",
    "display(abt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'medv', maxIter=10)\n",
    "lrModel = lr.fit(abt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the Coefficients\n",
    "\n",
    "beta = pd.DataFrame(np.array(lrModel.coefficients), columns=['betacoeff'])\n",
    "beta['coeffnames'] = features\n",
    "beta.sort_values(by='betacoeff', inplace=True)\n",
    "display(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model explains {}% of Total Variance\".format(round(lrModel.summary.r2*100)))\n",
    "\n",
    "print(\"β0 (intercept pval): {}\".format(lrModel.summary.pValues[0]))\n",
    "for i, (col, coef) in enumerate(zip(features, lrModel.summary.pValues[1:])):\n",
    "  if lrModel.summary.pValues[i] > 0.05:\n",
    "    print(\"β{} (coefficient pval for {}): {}\".format(i+1, col, coef))\n",
    "  else:\n",
    "    print(\"β{} coefficient not significant at 5%\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = lrModel.transform(abt_test)\n",
    "display(predictions.select('medv', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "predictionsDF = predictions.toPandas()\n",
    "sns.residplot('prediction', 'medv', data=predictionsDF)\n",
    "plt.xlabel(\"Predicted values for medv\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residual Plot\")\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"medv\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(\"rmse on the test set for the Linear model: {}\".format(rmse))\n",
    "print(\"mse on the test set for the Linear model: {}\".format(mse))\n",
    "print(\"r2 on the test set for the Linear model: {}\".format(r2))\n",
    "print(\"mae on the test set for the Linear model: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a series of Multivariate Linear Regression Experiments with MLflow\n",
    "Use MlFlow to record model, log model parameters, metrics, and artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tracking function\n",
    "def log_lineareg(experimentID, run_name, params, abt_train, abt_test, debug=False):\n",
    "  \n",
    "  \"\"\"\n",
    "  Function to start a run within a existing experiment\n",
    "  :param experimentID: unique ID associated to original experiment\n",
    "  :param run_name: label for the name of the run\n",
    "  :param params: ters used for the run, such as arguments\n",
    "  :param abt_train: analytical base table for training \n",
    "  :param abt_test: analytical base table for testing\n",
    "  :param debug: for debugging purpose\n",
    "  :return: run ID\n",
    "  \"\"\"\n",
    "\n",
    "  with mlflow.start_run(experiment_id=experimentID, run_name=run_name) as run:\n",
    "  \n",
    "  #Define variables\n",
    "#   params = {'featuresCol' : 'features', 'labelCol' : 'medv', 'maxIter' : 10}\n",
    "\n",
    "  # Create Model Instance\n",
    "    lr = LinearRegression(**params)\n",
    "    \n",
    "    if debug:\n",
    "      print(lr.params)\n",
    "\n",
    "    # Fit Model and Predict\n",
    "    lrModel = lr.fit(abt_train)\n",
    "    predictions = lrModel.transform(abt_test)\n",
    "\n",
    "    # Log params and metrics using the MLflow APIs\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "    #Log artefacts (Scored Test data & Coefficients Summary)\n",
    "\n",
    "    ## Scored Test data\n",
    "    temp1 = tempfile.NamedTemporaryFile(prefix='scored_df_', suffix='.csv')\n",
    "    temp1_name = temp1.name\n",
    "    try: \n",
    "      scored_df = predictions.drop('features').toPandas()\n",
    "      scored_df.to_csv(temp1_name, index=False)\n",
    "      mlflow.log_artifact(temp1_name)\n",
    "    except SystemError:\n",
    "      print('Check the log!')\n",
    "    finally:\n",
    "      temp1.close()\n",
    "\n",
    "    ## Coefficients Summary\n",
    "    temp2 = tempfile.NamedTemporaryFile(prefix='Coefficients_summary_', suffix='.csv')\n",
    "    temp2_name = temp2.name\n",
    "    try: \n",
    "      summary = pd.DataFrame(features, columns=['features'])\n",
    "      summary['betacoeff'] = np.array(lrModel.coefficients)\n",
    "      summary['pvalues'] = [round(pval, 4) for (col, pval) in zip(features, lrModel.summary.pValues[1:])]\n",
    "      summary.sort_values(by='pvalues', inplace=True)\n",
    "      summary.to_csv(temp2_name, index=False)\n",
    "      mlflow.log_artifact(temp2_name)\n",
    "    except SystemError:\n",
    "      print('Check the log!')\n",
    "    finally:\n",
    "      temp2.close()\n",
    "\n",
    "    # Log residuals using a temporary file\n",
    "    temp3 = tempfile.NamedTemporaryFile(prefix=\"residuals-\", suffix=\".png\")\n",
    "    temp3_name = temp3.name\n",
    "\n",
    "    try:\n",
    "      ## Create Residual plots\n",
    "      fig, ax = plt.subplots()\n",
    "      sns.residplot('prediction', 'medv', data=scored_df)\n",
    "      plt.xlabel(\"Predicted values for medv\")\n",
    "      plt.ylabel(\"Residual\")\n",
    "      plt.title(\"Residual Plot\")\n",
    "      fig.savefig(temp3_name)\n",
    "      mlflow.log_artifact(temp3_name, \"residuals.png\")\n",
    "\n",
    "    finally:\n",
    "      temp3.close() # Delete the temp file\n",
    "\n",
    "    # Log the model both in python and in spark and mleap flavors\n",
    "    mlflow.spark.log_model(spark_model=lrModel, \n",
    "                           artifact_path=\"pyspark-multi-linear-model\", \n",
    "                           sample_input=abt_test)\n",
    "\n",
    "    runID = run.info.run_uuid\n",
    "    experimentID = run.info.experiment_id\n",
    "\n",
    "    return runID\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1...2...n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'featuresCol' : 'features', 'labelCol' : 'medv', 'maxIter' : 10}\n",
    "\n",
    "log_lineareg(experimentID, '1th run', params, abt_train, abt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'featuresCol' : 'features', 'labelCol' : 'medv', 'maxIter' : 50}\n",
    "\n",
    "log_lineareg(experimentID, '2th run', params, abt_train, abt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'featuresCol' : 'features', 'labelCol' : 'medv', 'maxIter' : 50, 'fitIntercept': True, 'solver': 'normal'}\n",
    "\n",
    "log_lineareg(experimentID, '3rd run', params, abt_train, abt_test, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "Assume that:\n",
    "\n",
    "    1. you get the champion model (Lack in Model Registry functionality on current Databricks community edition). \n",
    "    2. And because IT asks you, you need to deploy in GCP with mleap flavor.\n",
    "    \n",
    "Then you would like to test locally the model and then pass it to ML engineering for implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we cannot load the MLeap model flavor in Python (https://mlflow.org/docs/latest/python_api/mlflow.mleap.html#module-mlflow.mleap), we have \n",
    "\n",
    "1. Download it using the Java API method downloadArtifacts(String runId) \n",
    "2. Load the model using the method MLeapLoader.loadPipeline(String modelRootPath)\n",
    "\n",
    "For reference: https://docs.databricks.com/applications/machine-learning/model-export-import/mleap-model-export.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "#Get the list of all the runs for last experiment\n",
    "client.list_run_infos(experimentID)\n",
    "\n",
    "#Store run info in a dataframe\n",
    "runs = pd.DataFrame([(run.run_uuid, run.start_time, run.artifact_uri) for run in client.list_run_infos(experimentID)])\n",
    "runs.columns = [\"run_uuid\", \"start_time\", \"artifact_uri\"]\n",
    "\n",
    "#Sort by start_time and pick the last run\n",
    "last_run = runs.sort_values(\"start_time\", ascending=False).iloc[0]\n",
    "\n",
    "dbutils.fs.ls(last_run[\"artifact_uri\"]+\"/pyspark-multi-linear-model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh \n",
    "rm -rf /tmp/mleap_python_model_export\n",
    "mkdir /tmp/mleap_python_model_export\n",
    "ls -la /tmp/mleap_python_model_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize Model to Bundle\n",
    "lrModel.serializeToBundle(\"jar:file:/tmp/mleap_python_model_export/lrModel.zip\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh \n",
    "ls -la /tmp/mleap_python_model_export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/tmp/mleap_python_model_export/lrModel.zip\", \"dbfs:/example/lrModel.zip\")\n",
    "display(dbutils.fs.ls(\"dbfs:/example\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeSerialize Model to Bundle\n",
    "deserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:/tmp/mleap_python_model_export/lrModel.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleResults = deserializedPipeline.transform(abt_test)\n",
    "display(exampleResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last but not least..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everythings works fine! \n",
    "\n",
    "At this point, Data Scientist generally **Packages the ML project**\n",
    "\n",
    "**BUT....**\n",
    "\n",
    "The main assumption here is : ***Batch Model Deployment in GCP with mleap flavor***\n",
    "\n",
    "Now, the primary use of MLeap is to import models into applications without Spark available. These applications should be implemented in Scala or Java\n",
    "\n",
    "This implies that the best artchitecture is the one that ***Consume Mleap model in a Scala (or use the mlflow/java package) job in the GCP dataproc***\n",
    "\n",
    "Anyway, **you can do that in Pyspark as well**\n",
    "\n",
    "So we need to **Move the Mleap flavor from local MLflow server to GCP**\n",
    "    \n",
    "Because you're planning on deployment in GCP Dataproc and Databricks does not provide an out-of-the-box deployment mechanism, you have two possible way to go: \n",
    "    \n",
    "        a. Git \n",
    "        b. Mlflow\n",
    "        \n",
    "    About (b), MLflow tracking server has two components for storage: a backend store and an artifact store. \n",
    "    The backend store is where MLflow Tracking Server stores experiment and run metadata as well as params, metrics, and tags for runs.\n",
    "    The artifact store is a location suitable for large data (such as an S3 bucket or shared NFS file system) and is where clients log their artifact output (for example, models).\n",
    "    \n",
    "    Then you can create a MLflow Tracking server on GCP (create a SQL instance for backend store and Google Cloud Storage for artifact store) and store information remotely by making use of Tracking URI option\n",
    "    \n",
    "Cool solution. \n",
    "\n",
    "But it misses CI/CD logics...So I decide to go with (a). And:\n",
    "\n",
    "    - Download the mleap flavor and push into a Git repo (because we're using the Databricks' Community edition. No Integration with GitHub)\n",
    "    \n",
    "    - Store it in Google Cloud Storage\n",
    "\n",
    "Work in progress: Consume the artefact in GitHub Actions (GitOps logics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the mleap flavor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLpackagePath = \"/FileStore/ModelProjects/Boston_ML\"\n",
    "dbutils.fs.rm(MLpackagePath, True)\n",
    "dbutils.fs.mkdirs(MLpackagePath)\n",
    "dbutils.fs.ls(MLpackagePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the environment\n",
    "# Copy data to score\n",
    "dbutils.fs.cp(\"dbfs:/data/boston_house_prices.csv\", \"dbfs:/FileStore/ModelProjects/Boston_ML\")\n",
    "# Copy model to consume for scoring\n",
    "dbutils.fs.cp(\"dbfs:/example/lrModel.zip\",\"dbfs:/FileStore/ModelProjects/Boston_ML\")\n",
    "# Check the content\n",
    "dbutils.fs.ls(MLpackagePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to download the Model Artefact, below you can find an example: \n",
    "\n",
    "https://community.cloud.databricks.com/files/ModelProjects/Boston_ML/lrModel.zip?o=5798411837794065\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "1_Development",
  "notebookId": 2146357741137526
 },
 "nbformat": 4,
 "nbformat_minor": 4
}